% !TEX root = ../thesis.tex
%
\chapter{Introduction}

For researchers studying consumer opinions and trends user generated content is becoming an increasingly important input. Based on the discussions underneath online editorial sources or on discussion boards social scientists can perform opinion analysis on a scale that is not possible with classical approaches.
The classical way of surveying consumers about their opinion on products or certain topics relies on voluntary questioning e.g. at supermarkets. These survey approaches have some drawbacks. First, it has to be made sure that the people questioned are representative for the larger population being studied. Second, in their response the participants might introduce a bias, since they know that they are being surveyed. Analyzing online user-generated data can help mitigate some of these drawbacks. Since the users do not know that their comments are used for opinion analyses there is less risk for bias. Further, any person has the ability to post online, which ensures that the comment sections and discussion boards contain a wide variety of opinions.

Even if user-generated content is used for qualitative studies, it is usually analyzed manually. With the huge growth of online text data, it is becoming of vital importance for social scientists to have reliable methods for fast automated analysis of such data. Among other things, researchers are interested in methods able to track topics, opinions, and sentiments in user-generated content \citep{Nikolenko2017}. Providing such a framework is the main objective of the \textit{SocialROM} project.

From a NLP perspective finding topics in large document collections is known as topic modeling. Topic models take the documents as an input and outputs topics and for every document a distribution specifying how it is composed of these topics. In topic modeling, a topic is a probability distribution over all words in the documents. By ranking the words according to their probability every topic consists of different words. However, the words of one topic refer to the same concept or theme. For example, a topic with the most probable words \textit{fish}, \textit{salmon}, \textit{wild} and \textit{seafood} refers to the \textit{fishing industry}. Each topic is a recurring theme that is discussed in the collection and is based on the co-occurrence of related words.

This project is done in cooperation with the chair of marketing research and consumer affairs at the Technical University of Munich. A part of their chair work on qualitative methods for social media analysis and opinion elicitation regarding sustainable consumption and products. Therefore, our analysis of user-generated data focuses on discussions regarding organic products. This subject is fitting since organic vs. conventional food is a widely discussed online with diverse opinions and sentiments.

In \textit{Generation 1} (\cite{Widmer2018}) firstly all data, which were relevant for our domain regarding organic food and products were scraped. Further information can be found in chapter \ref{dataset}. Then topics were generated, with the focus on finding the best topics and showing them to our domain experts. To generate the best topics different parameters for \acf{LDA} and \acf{NMF} were tried out and a method was developed to find the optimal number of topics per dataset. After creating the topics, a part of them was handed over to the domain experts and was labeled by them to evaluate, which datasets are meaningful. Based on the labels, the topic overlap  of every dataset was considered, and it was discovered, that the discussed issues from editorial articles are more similar to the topics identified in forum threads than to the topics of editorial comments and the topics from blogs are most similar to blog comments. Furthermore, the topic labels were compared with the results of a qualitative survey, where people were asked why the buy organic products. The given reasons were also reflected in the topics derived from online discussions. Analogously, to \cite{Griffiths2004} the development of topics over time was considered, to identify trends.

This thesis builds up on the topics from \textit{Generation 1}, theses were used to apply \acf{ATL} on them. The output of topic models are topics, which are represented with the top 10 words, sorted according the highest probability. It is desired, that the topics belong to a concept. For example from the top 5 words \textit{costs, price, food, product} and \textit{supermarket} it becomes apparent, that it is dealing with \textit{food prices}. Therefore, the label food prices is assigned to the topic. The label assignment has so far been only done by domain experts, which is very time consuming for them. Therefore, different procedures for the automatic allocation of labels to topics were tried out and compared, in order to relieve the domain experts or to support them in their work. This is also necessary if topic modeling is generated on a growing live corpus and new topics can be constantly added, e.g. online discussions, and the actual themes shall be shown.

After \ac{ATL} another main goal of this work is to prove the internal consistency, which means, to analyze how the topics itself and the distribution of topics on documents change when increasing the number of topics. Concretely, it shall be analyzed with different key figures whether the topics are getting specific or general and what specific and general in this context means. Furthermore, it shall be examined if the topics split up and whether this can be proven according to the top 10 words of the topic. All theses analyses shall provide domain experts the overview how topics change when increasing or decreasing the number of topics, so they can assess, which topic model is the most appropriate one for their expected results.
%\section{Research Objectives}
%	Wie baue ich darauf auf?
\section{Thesis structure}
First the methods, which were used to identify the topics are introduced in Chapter \ref{methodology}. This includes the approaches to represent the content of documents numerically and the algorithms for topic modeling with \acf{LDA} and \acf{NMF} .
In chapter \ref{dataset} we introduce the dataset and show how the data were gathered and preprocessed.
In the first half of Chapter 4, in Section \ref{automaticTL}, the possible approaches for \acl{ATL} are described and the results of applying those on our dataset are discussed. Accordingly, in the second half of Chapter 4, in Section \ref{Internal_consistency}, different key figures to measure the internal consistency are first introduced, applied and then discussed on our dataset.
Chapter \ref{future_work} completes the thesis by providing an outlook for possible future work and summarizes the thesis with the conclusion.

\newpage